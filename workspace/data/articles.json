[
  {
    "title_en": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
    "title_es": "GoT-R1: liberar la capacidad de razonamiento de MLLM para la generación visual con aprendizaje por refuerzo",
    "content_en": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "content_es": "Los modelos de generación visual han avanzado notablemente en la creación de imágenes realistas a partir de instrucciones de texto.\nimágenes realistas a partir de mensajes de texto, pero tienen dificultades con los mensajes complejos que especifican múltiples objetos con atributos espaciales precisos.\nmúltiples objetos con relaciones espaciales y atributos precisos. Para que\nque especifican múltiples objetos con relaciones espaciales precisas y atributos.\nsemántico y la disposición espacial. Presentamos GoT-R1, un marco que aplica el aprendizaje\npara mejorar el razonamiento semántico-espacial en la generación visual. En\nGoT-R1 permite a los modelos descubrir de forma autónoma estrategias de razonamiento eficaces que van más allá del razonamiento semántico.\ndescubrir de forma autónoma estrategias de razonamiento eficaces\npredefinidas mediante un aprendizaje por refuerzo cuidadosamente diseñado. Para lograrlo\nproponemos un marco de recompensa multidimensional de doble etapa que aprovecha los MLLM\npara evaluar tanto el proceso de razonamiento como el resultado final.\nsupervisión eficaz de todo el proceso de generación. El sistema de recompensas evalúa\nEl sistema de recompensa evalúa la alineación semántica, la precisión espacial y la calidad visual en un enfoque unificado.\nLos resultados experimentales demuestran mejoras significativas en T2I-CompBench\nsobre todo en tareas de composición que implican relaciones espaciales precisas y vinculación de atributos.\nespaciales precisas y vinculación de atributos. GoT-R1 avanza en el estado del arte de la generación de imágenes\ngeneración de imágenes transfiriendo con éxito sofisticadas\nal ámbito de la generación visual. Para facilitar futuras investigaciones\nnuestro código y los modelos preentrenados están a disposición del público en\nhttps://github.com/gogoduan/GoT-R1.",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "source_url": "http://arxiv.org/abs/2505.17022v1"
  },
  {
    "title_en": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework",
    "title_es": "Que los androides sueñen con ovejas eléctricas: Un marco de comprensión y razonamiento de la implicación de imágenes similares a las humanas",
    "content_en": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "content_es": "La comprensión metafórica de las imágenes sigue siendo un reto crucial para los sistemas de IA.\nya que los modelos existentes tienen dificultades para captar los matices culturales, emocionales y contextuales del contenido visual,\nculturales, emocionales y contextuales de los contenidos visuales. Aunque los\nde lenguaje multimodal (MLLM) destacan en tareas básicas de respuesta a preguntas visuales (VQA), se enfrentan a una limitación fundamental en la implicación de imágenes.\nuna limitación fundamental en las tareas de implicación de imágenes: las lagunas\nque ocultan las relaciones entre los distintos elementos visuales y sus significados abstractos.\nsignificados abstractos. Inspirándonos en el proceso cognitivo humano, proponemos Let\nAndroids Dream (LAD), un nuevo marco para la comprensión y el razonamiento de la implicación de imágenes.\ny razonamiento de imágenes. LAD aborda la falta de contexto a través de un marco de tres etapas:\n(1) Percepción: conversión de la información visual en representaciones textuales ricas y multinivel.\ntextuales ricas y multinivel, (2) Búsqueda: búsqueda iterativa e integración de conocimientos\npara resolver la ambigüedad, y (3) Razonamiento: generación de la implicación de la imagen en el contexto mediante el razonamiento explícito.\nimplicación de la imagen mediante razonamiento explícito. Nuestro marco con el modelo ligero\nGPT-4o-mini alcanza un rendimiento SOTA en comparación con más de 15 MLLM en la prueba de\nen inglés y una enorme mejora en chino,\nEl rendimiento es comparable al del modelo GPT-4o en preguntas de elección múltiple (MCQ).\ny supera en un 36,7% a la pregunta de estilo abierto (OSQ). Además, nuestro trabajo\nAdemás, nuestro trabajo aporta nuevos conocimientos sobre cómo la IA puede interpretar más eficazmente las implicaciones de las imágenes, lo que supone un avance en el campo de la visión.\nimplicaciones de la imagen, avanzando en el campo del razonamiento visión-lenguaje y la interacción\nhumana. Nuestro proyecto está a disposición del público en\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "source_url": "http://arxiv.org/abs/2505.17019v1"
  },
  {
    "title_en": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
    "title_es": "Profundizando en la RL para la generación de imágenes con CoT: Un estudio sobre DPO frente a GRPO",
    "content_en": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
    "content_es": "Los últimos avances subrayan el importante papel del aprendizaje por refuerzo\n(RL) en la mejora de las capacidades de razonamiento de la cadena de pensamiento (CoT) de los grandes modelos lingüísticos (LLM).\ngrandes modelos lingüísticos (LLM). Dos destacados algoritmos de RL, Direct Preference\n(OPD) y la Optimización de Política Relativa de Grupo (OPGR), son fundamentales\nde estos desarrollos, que presentan distintos pros y contras. Imagen autorregresiva\nLa generación autorregresiva de imágenes, también interpretable como un proceso de razonamiento CoT secuencial, presenta\ndesafíos únicos distintos del razonamiento CoT basado en LLM. Se trata de\ngarantizar la coherencia texto-imagen, mejorar la calidad estética de la imagen y\ndiseñar modelos de recompensa sofisticados, en lugar de basarse en recompensas\nrecompensas basadas en reglas. Aunque en los últimos tiempos se ha extendido la RL a este ámbito\nde este ámbito, estas exploraciones carecen normalmente de un análisis\ny las características de las distintas estrategias de RL. Para\nde los algoritmos GRPO y DPO en imágenes autorregresivas.\nen la generación autorregresiva de imágenes, evaluando su rendimiento en el ámbito\nevaluando su rendimiento en el dominio y su generalización fuera de él, y analizando el impacto de los distintos modelos de recompensa en sus capacidades respectivas.\ndiferentes modelos de recompensa en sus respectivas capacidades. Nuestros resultados revelan\nGRPO y DPO presentan distintas ventajas y, sobre todo, que los modelos de recompensa\nque poseen mayores capacidades intrínsecas de generalización potencialmente\npotencialmente el potencial de generalización de los algoritmos de RL aplicados. Además,\nAdemás, exploramos sistemáticamente tres estrategias de escalado prevalentes para mejorar tanto\nen el dominio y fuera del dominio, obteniendo información única sobre cómo escalar\nde cada paradigma. Esperamos que nuestro estudio\npara inspirar futuros trabajos sobre el desarrollo de algoritmos de RL más eficaces\npara lograr un razonamiento CoT robusto en el ámbito de la generación de imágenes autorregresivas.\nautorregresiva. El código está disponible en\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "source_url": "http://arxiv.org/abs/2505.17017v1"
  }
]