[
  {
    "title_en": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
    "title_es": "GoT-R1: liberar la capacidad de razonamiento de MLLM para la generación visual con aprendizaje por refuerzo",
    "title_zh": "GoT-R1：利用强化学习释放 MLLM 在视觉生成方面的推理能力",
    "content_en": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "content_es": "Los modelos de generación visual han avanzado notablemente en la creación de imágenes realistas a partir de instrucciones de texto.\nimágenes realistas a partir de mensajes de texto, pero tienen dificultades con los mensajes complejos que especifican múltiples objetos con atributos espaciales precisos.\nmúltiples objetos con relaciones espaciales y atributos precisos. Para que\nque especifican múltiples objetos con relaciones espaciales precisas y atributos.\nsemántico y la disposición espacial. Presentamos GoT-R1, un marco que aplica el aprendizaje\npara mejorar el razonamiento semántico-espacial en la generación visual. En\nGoT-R1 permite a los modelos descubrir de forma autónoma estrategias de razonamiento eficaces que van más allá del razonamiento semántico.\ndescubrir de forma autónoma estrategias de razonamiento eficaces\npredefinidas mediante un aprendizaje por refuerzo cuidadosamente diseñado. Para lograrlo\nproponemos un marco de recompensa multidimensional de doble etapa que aprovecha los MLLM\npara evaluar tanto el proceso de razonamiento como el resultado final.\nsupervisión eficaz de todo el proceso de generación. El sistema de recompensas evalúa\nEl sistema de recompensa evalúa la alineación semántica, la precisión espacial y la calidad visual en un enfoque unificado.\nLos resultados experimentales demuestran mejoras significativas en T2I-CompBench\nsobre todo en tareas de composición que implican relaciones espaciales precisas y vinculación de atributos.\nespaciales precisas y vinculación de atributos. GoT-R1 avanza en el estado del arte de la generación de imágenes\ngeneración de imágenes transfiriendo con éxito sofisticadas\nal ámbito de la generación visual. Para facilitar futuras investigaciones\nnuestro código y los modelos preentrenados están a disposición del público en\nhttps://github.com/gogoduan/GoT-R1.",
    "content_zh": "视觉生成模型在根据文本提示创建逼真图像方面取得了显著进展\n图像方面取得了显著进展，但在处理复杂的提示信息时却举步维艰。\n具有精确空间关系和属性的多个对象。要有效\n处理这类提示需要对语义内容和空间布局进行明确的推理。\n和空间布局。我们介绍了 GoT-R1，这是一个应用强化学习来增强语义空间分析能力的框架。\n学习来增强视觉生成中的语义空间推理能力。该框架\n生成思维链方法的基础上，GoT-R1 使模型能够\n自主发现有效的推理策略。\n通过精心设计的强化学习，GoT-R1 可使模型自主发现预定义模板之外的有效推理策略。为此\n我们提出了一个双阶段多维奖励框架，利用 MLLMs\n来评估推理过程和最终输出，从而实现对整个生成管道的有效监督。\n监督整个生成管道。奖励系统可评估\n语义一致性、空间准确性和视觉质量。\n实验结果表明，在 T2I-CompBench\n基准，特别是在涉及精确空间关系和属性绑定的合成任务中，有了明显的改进。\n关系和属性绑定。GoT-R1 通过成功地将复杂的推理转移到图像生成中，推动了\n图像生成领域的先进水平。\n功能转移到视觉生成领域，从而推进了图像生成领域的最新技术。为了促进未来的研究，我们\n为了方便未来的研究，我们将代码和预训练模型公开在\nhttps://github.com/gogoduan/GoT-R1.",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "source_url": "http://arxiv.org/abs/2505.17022v1",
    "image": "https://source.unsplash.com/600x400/?cs.CV,science"
  },
  {
    "title_en": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework",
    "title_es": "Que los androides sueñen con ovejas eléctricas: Un marco de comprensión y razonamiento de la implicación de imágenes similares a las humanas",
    "title_zh": "让机器人做电动羊的梦：类人图像暗示理解与推理框架",
    "content_en": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "content_es": "La comprensión metafórica de las imágenes sigue siendo un reto crucial para los sistemas de IA.\nya que los modelos existentes tienen dificultades para captar los matices culturales, emocionales y contextuales del contenido visual,\nculturales, emocionales y contextuales de los contenidos visuales. Aunque los\nde lenguaje multimodal (MLLM) destacan en tareas básicas de respuesta a preguntas visuales (VQA), se enfrentan a una limitación fundamental en la implicación de imágenes.\nuna limitación fundamental en las tareas de implicación de imágenes: las lagunas\nque ocultan las relaciones entre los distintos elementos visuales y sus significados abstractos.\nsignificados abstractos. Inspirándonos en el proceso cognitivo humano, proponemos Let\nAndroids Dream (LAD), un nuevo marco para la comprensión y el razonamiento de la implicación de imágenes.\ny razonamiento de imágenes. LAD aborda la falta de contexto a través de un marco de tres etapas:\n(1) Percepción: conversión de la información visual en representaciones textuales ricas y multinivel.\ntextuales ricas y multinivel, (2) Búsqueda: búsqueda iterativa e integración de conocimientos\npara resolver la ambigüedad, y (3) Razonamiento: generación de la implicación de la imagen en el contexto mediante el razonamiento explícito.\nimplicación de la imagen mediante razonamiento explícito. Nuestro marco con el modelo ligero\nGPT-4o-mini alcanza un rendimiento SOTA en comparación con más de 15 MLLM en la prueba de\nen inglés y una enorme mejora en chino,\nEl rendimiento es comparable al del modelo GPT-4o en preguntas de elección múltiple (MCQ).\ny supera en un 36,7% a la pregunta de estilo abierto (OSQ). Además, nuestro trabajo\nAdemás, nuestro trabajo aporta nuevos conocimientos sobre cómo la IA puede interpretar más eficazmente las implicaciones de las imágenes, lo que supone un avance en el campo de la visión.\nimplicaciones de la imagen, avanzando en el campo del razonamiento visión-lenguaje y la interacción\nhumana. Nuestro proyecto está a disposición del público en\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "content_zh": "图像中的隐喻理解仍然是人工智能系统面临的一项重大挑战。\n系统的关键挑战，因为现有模型难以把握视觉内容中蕴含的细微的文化、情感和语境含义、\n和语境含义。虽然多模态大\n语言模型（MLLMs）在基本的视觉问题解答（VQA）任务中表现出色，但在图像理解方面却受到了根本性的限制。\n在图像蕴含任务中却面临着根本性的限制：语境\n间隙会模糊不同视觉元素之间的关系及其抽象含义。\n抽象含义之间的关系。受人类认知过程的启发，我们提出了让\nAndroids Dream (LAD)，这是一种新颖的图像蕴含理解和推理框架。\n推理框架。LAD 通过三个阶段的框架来解决语境缺失问题：\n(1) 感知：将视觉信息转换为丰富的多层次文本表征\n表征，(2) 搜索：迭代搜索和整合跨领域知识以解决模糊问题，以及\n知识以解决模糊问题，以及 (3) 推理：通过显式推理生成上下文对齐的\n图像暗示。我们的框架采用轻量级\nGPT-4o-mini 模型的框架在英语图像蕴涵基准测试中与 15 个以上的 MLLMs 相比取得了 SOTA 性能，并大大提高了图像蕴涵基准测试的性能。\n图像蕴含基准的 SOTA 性能，并在中文基准上取得了巨大进步、\n在多选题（MCQ）上的表现与 GPT-4o 模型不相上下\n在开放式问题（OSQ）上的表现优于 GPT-4o 模型 36.7%。此外，我们的工作\n还为人工智能如何更有效地解释图像\n意义的新见解，推动了视觉语言推理和人机交互领域的发展。\n交互领域。我们的项目可在\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "source_url": "http://arxiv.org/abs/2505.17019v1",
    "image": "https://source.unsplash.com/600x400/?cs.CV,science"
  },
  {
    "title_en": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
    "title_es": "Profundizando en la RL para la generación de imágenes con CoT: Un estudio sobre DPO frente a GRPO",
    "title_zh": "深入研究用 CoT 生成图像的 RL：关于 DPO 与 GRPO 的研究",
    "content_en": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
    "content_es": "Los últimos avances subrayan el importante papel del aprendizaje por refuerzo\n(RL) en la mejora de las capacidades de razonamiento de la cadena de pensamiento (CoT) de los grandes modelos lingüísticos (LLM).\ngrandes modelos lingüísticos (LLM). Dos destacados algoritmos de RL, Direct Preference\n(OPD) y la Optimización de Política Relativa de Grupo (OPGR), son fundamentales\nde estos desarrollos, que presentan distintos pros y contras. Imagen autorregresiva\nLa generación autorregresiva de imágenes, también interpretable como un proceso de razonamiento CoT secuencial, presenta\ndesafíos únicos distintos del razonamiento CoT basado en LLM. Se trata de\ngarantizar la coherencia texto-imagen, mejorar la calidad estética de la imagen y\ndiseñar modelos de recompensa sofisticados, en lugar de basarse en recompensas\nrecompensas basadas en reglas. Aunque en los últimos tiempos se ha extendido la RL a este ámbito\nde este ámbito, estas exploraciones carecen normalmente de un análisis\ny las características de las distintas estrategias de RL. Para\nde los algoritmos GRPO y DPO en imágenes autorregresivas.\nen la generación autorregresiva de imágenes, evaluando su rendimiento en el ámbito\nevaluando su rendimiento en el dominio y su generalización fuera de él, y analizando el impacto de los distintos modelos de recompensa en sus capacidades respectivas.\ndiferentes modelos de recompensa en sus respectivas capacidades. Nuestros resultados revelan\nGRPO y DPO presentan distintas ventajas y, sobre todo, que los modelos de recompensa\nque poseen mayores capacidades intrínsecas de generalización potencialmente\npotencialmente el potencial de generalización de los algoritmos de RL aplicados. Además,\nAdemás, exploramos sistemáticamente tres estrategias de escalado prevalentes para mejorar tanto\nen el dominio y fuera del dominio, obteniendo información única sobre cómo escalar\nde cada paradigma. Esperamos que nuestro estudio\npara inspirar futuros trabajos sobre el desarrollo de algoritmos de RL más eficaces\npara lograr un razonamiento CoT robusto en el ámbito de la generación de imágenes autorregresivas.\nautorregresiva. El código está disponible en\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
    "content_zh": "强化学习（RL）在增强大型思维链（CoT）推理能力方面的重要作用。\n(RL) 在增强大型语言模型 (LLM) 的思维链 (CoT) 推理能力方面的重要作用。\n语言模型（LLM）的推理能力方面发挥着重要作用。两种著名的强化学习算法，即直接偏好优化（DPO\n优化（DPO）和组相对策略优化（GRPO）是这些发展的核心。\n这两种著名的 RL 算法是这些发展的核心，它们展示了不同的优缺点。自回归图像\n自回归图像生成也可解释为一个顺序 CoT 推理过程，它提出了与基于 LLM 的自回归图像生成不同的独特挑战。\n与基于 LLM 的 CoT 推理不同，自回归图像生成面临着独特的挑战。这些挑战包括\n确保文本与图像的一致性，提高图像的美学质量，以及\n设计复杂的奖励模型，而不是依赖于简单的\n基于规则的奖励。虽然最近的努力已将 RL 扩展到这一领域，但这些\n探索通常缺乏对特定领域挑战和不同 RL 特征的深入分析。\n挑战和不同 RL 策略的特点。为了弥补这一\n为弥补这一不足，我们首次全面研究了自回归算法中的 GRPO 和 DPO\n算法进行了首次全面研究，评估了它们在自回归图像生成领域的\n性能和域外泛化，同时仔细研究了不同奖励模型对它们各自能力的影响。\n不同奖励模型对它们各自能力的影响。我们的研究结果表明\n我们的研究结果表明，GRPO 和 DPO 表现出截然不同的优势，最重要的是，奖励模型具有更强的内在泛化能力。\n奖励模型具有更强的内在泛化能力，有可能\n增强了所应用的 RL 算法的泛化潜力。此外\n我们还系统地探索了三种流行的扩展策略，以增强\n此外，我们还系统地探索了三种流行的扩展策略，以提高它们在域内和域外的熟练程度，并就每种范式的高效扩展性能获得了独特的见解。\n高效扩展每种范式的性能的独特见解。我们希望我们的研究能为\n我们希望我们的研究能为今后开发更有效的 RL 算法铺平新的道路。\n在自回归图像生成领域实现稳健的 CoT 推理。\n生成。代码发布于\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "source_url": "http://arxiv.org/abs/2505.17017v1",
    "image": "https://source.unsplash.com/600x400/?cs.CV,science"
  },
  {
    "title_en": "Interactive Post-Training for Vision-Language-Action Models",
    "title_es": "Postformación interactiva para modelos de visión, lenguaje y acción",
    "title_zh": "视觉-语言-动作模型的交互式后期训练",
    "content_en": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "content_es": "Presentamos RIPT-VLA, un paradigma de post-entrenamiento interactivo, simple y escalable, basado en el aprendizaje por refuerzo.\nbasado en el aprendizaje por refuerzo que afina los modelos\n(VLA) preentrenados utilizando únicamente recompensas de éxito binarias dispersas.\nLas líneas de entrenamiento de VLA existentes se basan en gran medida en los datos de demostración de expertos offline y en la imitación supervisada, lo que limita la eficacia de los modelos.\ny la imitación supervisada, lo que limita su capacidad para adaptarse a nuevas tareas y entornos en regímenes de datos reducidos.\nnuevos entornos en regímenes con pocos datos. RIPT-VLA aborda este problema permitiendo\npost-entrenamiento interactivo con un algoritmo de optimización de política estable basado en\nmuestreo dinámico de despliegue y estimación de la ventaja de dejar uno fuera.\n  RIPT-VLA tiene las siguientes características. En primer lugar, se aplica a varios modelos VLA\nresultando en una mejora del modelo ligero QueST en un 21,2%,\ny el modelo 7B OpenVLA-OFT a una tasa de éxito sin precedentes del 97,5%. En segundo lugar\nes eficiente desde el punto de vista computacional y de datos: con una sola demostración,\nRIPT-VLA permite que un modelo SFT inviable (4%) tenga un índice de éxito del 97% en 15 iteraciones.\nen 15 iteraciones. Además, demostramos que la política aprendida\npor RIPT-VLA se generaliza a través de diferentes tareas y escenarios y es robusta al\nel contexto del estado inicial. Estos resultados ponen de relieve que RIPT-VLA es un paradigma práctico y\nparadigma práctico y eficaz para el post-entrenamiento de modelos VLA mediante una supervisión mínima.",
    "content_zh": "我们介绍了 RIPT-VLA，这是一种基于强化学习的简单且可扩展的\n交互式后训练范例，该范例仅使用稀疏的二进制成功奖励来微调预训练的\n视觉-语言-动作（VLA）模型。\n现有的视觉语言动作（VLA）训练管道严重依赖离线专家演示数据和监督模仿\n数据和监督模仿，从而限制了它们在低数据环境下适应新任务和新环境的能力。\n新任务和环境的能力。RIPT-VLA 通过实现\n交互式后训练，并采用基于动态滚动采样和留一的稳定策略优化算法。\nRIPT-VLA 可通过基于动态滚动采样和留一优势估计的稳定策略优化算法实现交互式后训练，从而解决这一问题。\n  RIPT-VLA 具有以下特点。首先，它适用于各种 VLA\n首先，它适用于各种 VLA 模型，使轻量级 QueST 模型的性能提高了 21.2%、\n而 7B OpenVLA-OFT 模型的成功率则达到了前所未有的 97.5%。其次，它\n其次，它具有计算效率和数据效率：只需一次演示、\nRIPT-VLA 就能在 15 次迭代内，将无法运行的 SFT 模型（4%）成功率提高到 97%。\n成功率高达 97%。此外，我们还证明了\n还证明了 RIPT-VLA 所学习到的策略可在不同任务和场景中通用，并对初始状态环境具有鲁棒性。\n初始状态背景。这些结果突出表明，RIPT-VLA 是一种实用、有效的\n的范例。",
    "date": "2025-05-22",
    "category": "cs.LG",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "source_url": "http://arxiv.org/abs/2505.17016v1",
    "image": "https://source.unsplash.com/600x400/?cs.LG,science"
  },
  {
    "title_en": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding",
    "title_es": "SpatialScore: Hacia una evaluación unificada de la comprensión espacial multimodal",
    "title_zh": "空间分数：实现多模态空间理解的统一评估",
    "content_en": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
    "content_es": "Los modelos multimodales de gran lenguaje (MLLM) han logrado un éxito impresionante en tareas de respuesta a preguntas.\nen tareas de respuesta a preguntas, pero sus capacidades de comprensión espacial están\nmenos exploradas. Este trabajo investiga una cuestión fundamental: ¿poseen los MLLM existentes\nposeen capacidades de percepción y comprensión espacial en 3D? En concreto\nlas siguientes contribuciones (i) introducimos VGBench, un\n(i) introducimos VGBench, un benchmark diseñado específicamente para evaluar la percepción de la geometría visual de los MLLM,\n(ii) proponemos SpatialScore, la herramienta multimodal más completa y diversa para la evaluación de la percepción de la geometría visual.\n(ii) proponemos SpatialScore, la prueba multimodal de comprensión espacial más completa y diversa hasta la fecha,\nintegrando VGBench con datos relevantes de los otros 11 conjuntos de datos existentes.\nEste punto de referencia incluye 28.000 muestras de varias tareas de comprensión espacial, modalidades y QA.\nde comprensión espacial, modalidades y formatos de control de calidad, junto con un subconjunto\nSpatialScore-Hard; iii) desarrollamos SpatialAgent, un novedoso sistema multiagente\nmultiagente que incorpora 9 herramientas especializadas para la\nlos paradigmas de razonamiento Plan-Execute y ReAct.\nevaluaciones para revelar los retos persistentes en el razonamiento espacial y\nSpatialAgent. Creemos que SpatialScore\ncomo punto de referencia riguroso para la próxima evolución de los MLLM.\nevolución de los MLLM.",
    "content_zh": "多模态大语言模型（MLLMs）在问题解答任务中取得了令人瞩目的成就。\n然而，它们在空间理解方面的能力却鲜有探索。\n空间理解能力的探索却较少。这项工作研究了一个关键问题：现有的多模态大语言模型是否具备三维空间感知和理解能力？\n是否具备三维空间感知和理解能力？具体来说，我们\n本文的具体贡献如下：(i) 我们引入了 VGBench，这是一个专门用于评估 MLLMs 的三维空间感知和理解能力的基准。\n(i) 我们介绍了 VGBench，这是一个专门用于评估 MLLM 视觉几何感知能力的基准、\n例如，相机姿态和运动估计；(ii) 我们提出了 SpatialScore，它是最全面、最多样化的多模态空间感知基准。\nSpatialScore 是迄今为止最全面、最多样化的多模态空间理解基准、\nSpatialScore 是迄今为止最全面、最多样化的多模态空间理解基准，它将 VGBench 与其他 11 个现有数据集的相关数据整合在一起。\n该基准包括 28K 个样本，涵盖各种空间理解任务、模式和 QA\n任务、模式和 QA 格式的 28K 个样本，以及精心策划的具有挑战性的\n子集 SpatialScore-Hard；(iii) 我们开发了 SpatialAgent，这是一种新颖的多代理\n空间理解的 9 种专用工具，支持\n(iv) 我们进行了广泛的\n(iv) 我们进行了广泛的评估，以揭示空间推理中持续存在的挑战，同时\nSpatialAgent 的有效性。我们相信 SpatialScore 将\n我们相信 SpatialScore 将提供有价值的见解，并成为下一个\n我们相信 SpatialScore 将提供有价值的见解，并成为下一代 MLLM 的严格基准。",
    "date": "2025-05-22",
    "category": "cs.CV",
    "tags": [
      "cs.CV",
      "cs.AI"
    ],
    "source_url": "http://arxiv.org/abs/2505.17012v1",
    "image": "https://source.unsplash.com/600x400/?cs.CV,science"
  }
]